{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import sys\n",
    "import logging\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import  OneHotEncoder\n",
    "from gensim.models import Word2Vec\n",
    "import gensim\n",
    "\n",
    "from numpy import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import *\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from MultTensorDataset import MultTensorDataset\n",
    "\n",
    "train_path = 'data/train_preliminary/'\n",
    "test_path = 'data/test/'\n",
    "id_seq_path = './id_seq_path/'\n",
    "w2v_model_path = './w2v_model/'\n",
    "# feat = ['creative_id','ad_id','advertiser_id','product_id','product_category','industry']\n",
    "feat = ['creative_id','ad_id','advertiser_id','product_id']\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 3\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载预训练的word2vec模型，构建word2id、embedding矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_w2v(feat):\n",
    "    print(w2v_model_path+'word2vec_'+ feat +'_128_win100.model')\n",
    "    w2v_model = Word2Vec.load(w2v_model_path+'word2vec_'+ feat +'_128_win100.model')\n",
    "    print(w2v_model)\n",
    "    vocab_list = [word for word, Vocab in w2v_model.wv.vocab.items()]\n",
    "#     word_index = {\" \": 0} \n",
    "#     word_vector = {}\n",
    "    embedding_matrix = np.zeros((len(vocab_list) + 1, w2v_model.vector_size))\n",
    "    for i in range(len(vocab_list)):\n",
    "        word = vocab_list[i]  # 每个词语\n",
    "#         word_index[word] = i + 1 # 词语：索引\n",
    "#         word_vector[word] = w2v_model.wv[word] # 词语：词向量\n",
    "        embedding_matrix[i + 1] = w2v_model.wv[word]  # 词向量矩阵\n",
    "    print(embedding_matrix.shape)\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>user_id</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>creative_id_int_seq</th>\n",
       "      <th>ad_id_int_seq</th>\n",
       "      <th>advertiser_id_int_seq</th>\n",
       "      <th>product_id_int_seq</th>\n",
       "      <th>product_category_int_seq</th>\n",
       "      <th>industry_int_seq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>[5334, 181951, 39292, 334379, 36192, 74908, 19...</td>\n",
       "      <td>[1049, 284668, 44862, 603979, 188040, 22738, 1...</td>\n",
       "      <td>[8, 467, 3075, 5015, 1515, 335, 619, 1030, 833...</td>\n",
       "      <td>[3, 3, 243, 3, 381, 3, 3, 3, 3, 301, 7, 301, 76]</td>\n",
       "      <td>[1, 4, 2, 4, 2, 4, 4, 1, 4, 2, 2, 2, 2]</td>\n",
       "      <td>[22, 22, 1, 87, 19, 22, 8, 160, 102, 75, 1, 75...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>[581925, 112993, 6376, 169764, 2210, 37732, 39...</td>\n",
       "      <td>[166404, 1191511, 5632, 384949, 41, 117126, 74...</td>\n",
       "      <td>[18117, 2356, 381, 1138, 1, 35, 7418, 1264, 36...</td>\n",
       "      <td>[3, 571, 58, 144, 28, 7, 975, 3, 3, 59, 3, 599...</td>\n",
       "      <td>[1, 2, 2, 2, 2, 2, 5, 1, 1, 2, 1, 2, 2, 2, 2, ...</td>\n",
       "      <td>[16, 21, 23, 19, 48, 1, 138, 49, 15, 1, 15, 66...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>[2366, 100352, 426, 3129236, 36122, 2039703, 8...</td>\n",
       "      <td>[2226, 506474, 1332, 2053295, 13715, 1703510, ...</td>\n",
       "      <td>[1960, 1001, 290, 11847, 890, 1880, 393, 17110...</td>\n",
       "      <td>[57, 247, 23, 57, 946, 235, 98, 3, 925, 3, 74,...</td>\n",
       "      <td>[2, 2, 2, 2, 3, 2, 3, 1, 3, 1, 2, 7, 1, 1, 1, ...</td>\n",
       "      <td>[23, 3, 17, 23, 9, 17, 9, 90, 9, 13, 17, 108, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>[571253, 13851, 127701, 4640, 272910, 1886, 79...</td>\n",
       "      <td>[833884, 10725, 35771, 120836, 996418, 3536, 5...</td>\n",
       "      <td>[9564, 28, 781, 1657, 15387, 99, 2971, 3494, 2...</td>\n",
       "      <td>[1380, 7, 113, 3, 3796, 227, 3, 437, 3, 7, 547...</td>\n",
       "      <td>[3, 2, 2, 11, 6, 3, 1, 2, 1, 2, 3, 2, 3, 2, 2,...</td>\n",
       "      <td>[9, 1, 20, 9, 9, 9, 26, 19, 25, 1, 9, 1, 9, 23...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>[1174834, 320221, 3686, 1278551, 98010, 209486...</td>\n",
       "      <td>[1043708, 25759, 6961, 123891, 28270, 134334, ...</td>\n",
       "      <td>[1301, 5343, 60, 721, 3269, 5275, 6544, 4330, ...</td>\n",
       "      <td>[3, 243, 3, 3, 3, 243, 333, 3, 3, 83, 243, 3, ...</td>\n",
       "      <td>[4, 2, 1, 1, 1, 2, 2, 4, 1, 2, 2, 1, 2, 4, 1, ...</td>\n",
       "      <td>[52, 1, 23, 1, 104, 1, 17, 34, 23, 4, 1, 91, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899995</th>\n",
       "      <td>899995</td>\n",
       "      <td>899996</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>[267119, 21159, 1298151, 21159, 84204, 21874, ...</td>\n",
       "      <td>[166677, 963, 447575, 963, 162772, 6543, 21082...</td>\n",
       "      <td>[478, 221, 1023, 221, 1023, 203, 251, 61, 53, ...</td>\n",
       "      <td>[3, 113, 3, 113, 3, 3, 3, 59, 70, 3, 3, 3, 290...</td>\n",
       "      <td>[1, 2, 1, 2, 1, 1, 4, 2, 2, 1, 1, 1, 2, 1]</td>\n",
       "      <td>[7, 20, 13, 20, 13, 13, 33, 1, 1, 13, 13, 7, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899996</th>\n",
       "      <td>899996</td>\n",
       "      <td>899997</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[53, 46698, 18119, 11727, 62432, 2033, 98563, ...</td>\n",
       "      <td>[256, 68111, 22936, 1241, 7172, 1618, 84637, 3...</td>\n",
       "      <td>[7, 273, 659, 1, 5271, 1884, 1591, 392, 2335, ...</td>\n",
       "      <td>[190, 3, 35, 28, 822, 153, 146, 146, 319, 3, 2...</td>\n",
       "      <td>[2, 4, 9, 2, 2, 2, 2, 2, 2, 1, 2, 2, 9, 2, 2, ...</td>\n",
       "      <td>[21, 11, 27, 48, 100, 28, 1, 1, 23, 1, 48, 45,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899997</th>\n",
       "      <td>899997</td>\n",
       "      <td>899998</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>[160, 810, 94391, 10133, 2838, 140133, 3445, 9...</td>\n",
       "      <td>[513, 1232, 288797, 27957, 2259, 559602, 2253,...</td>\n",
       "      <td>[203, 10, 4, 8, 223, 8, 359, 196, 29, 8, 8, 41...</td>\n",
       "      <td>[3, 3, 3, 3, 177, 3, 225, 32, 3, 3, 3, 2257, 3...</td>\n",
       "      <td>[7, 7, 1, 1, 3, 1, 3, 9, 1, 1, 1, 3, 1, 1]</td>\n",
       "      <td>[13, 81, 1, 1, 9, 1, 9, 1, 18, 1, 1, 9, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899998</th>\n",
       "      <td>899998</td>\n",
       "      <td>899999</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[8338, 117182, 849974, 127037, 28196, 11726, 1...</td>\n",
       "      <td>[140345, 60679, 318630, 20807, 27320, 6491, 72...</td>\n",
       "      <td>[210, 860, 612, 1082, 1890, 12951, 1790, 8, 44...</td>\n",
       "      <td>[3, 3, 3, 60, 3, 2968, 3, 3, 3, 3, 3, 281, 3, ...</td>\n",
       "      <td>[1, 4, 4, 2, 4, 3, 5, 1, 4, 10, 1, 2, 1, 2, 1,...</td>\n",
       "      <td>[13, 14, 57, 24, 10, 9, 137, 1, 69, 9, 13, 21,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899999</th>\n",
       "      <td>899999</td>\n",
       "      <td>900000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[47370, 1498, 757800, 59375, 2062, 1282639, 71...</td>\n",
       "      <td>[34570, 298, 587489, 37998, 2740, 1445147, 345...</td>\n",
       "      <td>[621, 1387, 20495, 8, 8, 1890, 860, 2408, 4874...</td>\n",
       "      <td>[3, 350, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]</td>\n",
       "      <td>[4, 4, 1, 1, 1, 4, 4, 4, 4, 1, 1, 4]</td>\n",
       "      <td>[8, 11, 26, 88, 11, 10, 14, 59, 63, 25, 24, 63]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>900000 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0  user_id  age  gender  \\\n",
       "0                0        1    3       0   \n",
       "1                1        2    9       0   \n",
       "2                2        3    6       1   \n",
       "3                3        4    4       0   \n",
       "4                4        5    3       0   \n",
       "...            ...      ...  ...     ...   \n",
       "899995      899995   899996    4       0   \n",
       "899996      899996   899997    2       1   \n",
       "899997      899997   899998    3       1   \n",
       "899998      899998   899999    2       0   \n",
       "899999      899999   900000    2       1   \n",
       "\n",
       "                                      creative_id_int_seq  \\\n",
       "0       [5334, 181951, 39292, 334379, 36192, 74908, 19...   \n",
       "1       [581925, 112993, 6376, 169764, 2210, 37732, 39...   \n",
       "2       [2366, 100352, 426, 3129236, 36122, 2039703, 8...   \n",
       "3       [571253, 13851, 127701, 4640, 272910, 1886, 79...   \n",
       "4       [1174834, 320221, 3686, 1278551, 98010, 209486...   \n",
       "...                                                   ...   \n",
       "899995  [267119, 21159, 1298151, 21159, 84204, 21874, ...   \n",
       "899996  [53, 46698, 18119, 11727, 62432, 2033, 98563, ...   \n",
       "899997  [160, 810, 94391, 10133, 2838, 140133, 3445, 9...   \n",
       "899998  [8338, 117182, 849974, 127037, 28196, 11726, 1...   \n",
       "899999  [47370, 1498, 757800, 59375, 2062, 1282639, 71...   \n",
       "\n",
       "                                            ad_id_int_seq  \\\n",
       "0       [1049, 284668, 44862, 603979, 188040, 22738, 1...   \n",
       "1       [166404, 1191511, 5632, 384949, 41, 117126, 74...   \n",
       "2       [2226, 506474, 1332, 2053295, 13715, 1703510, ...   \n",
       "3       [833884, 10725, 35771, 120836, 996418, 3536, 5...   \n",
       "4       [1043708, 25759, 6961, 123891, 28270, 134334, ...   \n",
       "...                                                   ...   \n",
       "899995  [166677, 963, 447575, 963, 162772, 6543, 21082...   \n",
       "899996  [256, 68111, 22936, 1241, 7172, 1618, 84637, 3...   \n",
       "899997  [513, 1232, 288797, 27957, 2259, 559602, 2253,...   \n",
       "899998  [140345, 60679, 318630, 20807, 27320, 6491, 72...   \n",
       "899999  [34570, 298, 587489, 37998, 2740, 1445147, 345...   \n",
       "\n",
       "                                    advertiser_id_int_seq  \\\n",
       "0       [8, 467, 3075, 5015, 1515, 335, 619, 1030, 833...   \n",
       "1       [18117, 2356, 381, 1138, 1, 35, 7418, 1264, 36...   \n",
       "2       [1960, 1001, 290, 11847, 890, 1880, 393, 17110...   \n",
       "3       [9564, 28, 781, 1657, 15387, 99, 2971, 3494, 2...   \n",
       "4       [1301, 5343, 60, 721, 3269, 5275, 6544, 4330, ...   \n",
       "...                                                   ...   \n",
       "899995  [478, 221, 1023, 221, 1023, 203, 251, 61, 53, ...   \n",
       "899996  [7, 273, 659, 1, 5271, 1884, 1591, 392, 2335, ...   \n",
       "899997  [203, 10, 4, 8, 223, 8, 359, 196, 29, 8, 8, 41...   \n",
       "899998  [210, 860, 612, 1082, 1890, 12951, 1790, 8, 44...   \n",
       "899999  [621, 1387, 20495, 8, 8, 1890, 860, 2408, 4874...   \n",
       "\n",
       "                                       product_id_int_seq  \\\n",
       "0        [3, 3, 243, 3, 381, 3, 3, 3, 3, 301, 7, 301, 76]   \n",
       "1       [3, 571, 58, 144, 28, 7, 975, 3, 3, 59, 3, 599...   \n",
       "2       [57, 247, 23, 57, 946, 235, 98, 3, 925, 3, 74,...   \n",
       "3       [1380, 7, 113, 3, 3796, 227, 3, 437, 3, 7, 547...   \n",
       "4       [3, 243, 3, 3, 3, 243, 333, 3, 3, 83, 243, 3, ...   \n",
       "...                                                   ...   \n",
       "899995  [3, 113, 3, 113, 3, 3, 3, 59, 70, 3, 3, 3, 290...   \n",
       "899996  [190, 3, 35, 28, 822, 153, 146, 146, 319, 3, 2...   \n",
       "899997  [3, 3, 3, 3, 177, 3, 225, 32, 3, 3, 3, 2257, 3...   \n",
       "899998  [3, 3, 3, 60, 3, 2968, 3, 3, 3, 3, 3, 281, 3, ...   \n",
       "899999             [3, 350, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]   \n",
       "\n",
       "                                 product_category_int_seq  \\\n",
       "0                 [1, 4, 2, 4, 2, 4, 4, 1, 4, 2, 2, 2, 2]   \n",
       "1       [1, 2, 2, 2, 2, 2, 5, 1, 1, 2, 1, 2, 2, 2, 2, ...   \n",
       "2       [2, 2, 2, 2, 3, 2, 3, 1, 3, 1, 2, 7, 1, 1, 1, ...   \n",
       "3       [3, 2, 2, 11, 6, 3, 1, 2, 1, 2, 3, 2, 3, 2, 2,...   \n",
       "4       [4, 2, 1, 1, 1, 2, 2, 4, 1, 2, 2, 1, 2, 4, 1, ...   \n",
       "...                                                   ...   \n",
       "899995         [1, 2, 1, 2, 1, 1, 4, 2, 2, 1, 1, 1, 2, 1]   \n",
       "899996  [2, 4, 9, 2, 2, 2, 2, 2, 2, 1, 2, 2, 9, 2, 2, ...   \n",
       "899997         [7, 7, 1, 1, 3, 1, 3, 9, 1, 1, 1, 3, 1, 1]   \n",
       "899998  [1, 4, 4, 2, 4, 3, 5, 1, 4, 10, 1, 2, 1, 2, 1,...   \n",
       "899999               [4, 4, 1, 1, 1, 4, 4, 4, 4, 1, 1, 4]   \n",
       "\n",
       "                                         industry_int_seq  \n",
       "0       [22, 22, 1, 87, 19, 22, 8, 160, 102, 75, 1, 75...  \n",
       "1       [16, 21, 23, 19, 48, 1, 138, 49, 15, 1, 15, 66...  \n",
       "2       [23, 3, 17, 23, 9, 17, 9, 90, 9, 13, 17, 108, ...  \n",
       "3       [9, 1, 20, 9, 9, 9, 26, 19, 25, 1, 9, 1, 9, 23...  \n",
       "4       [52, 1, 23, 1, 104, 1, 17, 34, 23, 4, 1, 91, 2...  \n",
       "...                                                   ...  \n",
       "899995  [7, 20, 13, 20, 13, 13, 33, 1, 1, 13, 13, 7, 2...  \n",
       "899996  [21, 11, 27, 48, 100, 28, 1, 1, 23, 1, 48, 45,...  \n",
       "899997      [13, 81, 1, 1, 9, 1, 9, 1, 18, 1, 1, 9, 1, 1]  \n",
       "899998  [13, 14, 57, 24, 10, 9, 137, 1, 69, 9, 13, 21,...  \n",
       "899999    [8, 11, 26, 88, 11, 10, 14, 59, 63, 25, 24, 63]  \n",
       "\n",
       "[900000 rows x 10 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_seq_data = pd.read_csv(id_seq_path+'df_all_id_seq.csv')\n",
    "for f in feat:\n",
    "    df_seq_data[f+'_int_seq'] = df_seq_data[f+'_int_seq'].apply(lambda x: eval(x))\n",
    "df_seq_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "划分训练集和验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_dev = train_test_split(df_seq_data, test_size=0.1, random_state=2020)\n",
    "y_train = df_train['age']\n",
    "x_train = df_train.drop(['gender', 'age', 'user_id'], axis=1)\n",
    "y_dev = df_dev['age']\n",
    "x_dev = df_dev.drop(['gender', 'age', 'user_id'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构造dataloader和定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MultTensorDataset(feat, x_train, y_train.values)\n",
    "train_dataloader = DataLoader(dataset = train_dataset, batch_size = BATCH_SIZE, shuffle = True, num_workers = 4)\n",
    "\n",
    "dev_dataset = MultTensorDataset(feat, x_dev, y_dev.values)\n",
    "dev_dataloader = DataLoader(dataset = dev_dataset, batch_size = BATCH_SIZE, shuffle = True, num_workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./w2v_model/word2vec_creative_id_128_win100.model\n",
      "Word2Vec(vocab=3412772, size=128, alpha=0.025)\n",
      "(3412773, 128)\n",
      "./w2v_model/word2vec_ad_id_128_win100.model\n",
      "Word2Vec(vocab=3027360, size=128, alpha=0.025)\n",
      "(3027361, 128)\n",
      "./w2v_model/word2vec_advertiser_id_128_win100.model\n",
      "Word2Vec(vocab=57870, size=128, alpha=0.025)\n",
      "(57871, 128)\n",
      "./w2v_model/word2vec_product_id_128_win100.model\n",
      "Word2Vec(vocab=39057, size=128, alpha=0.025)\n",
      "(39058, 128)\n"
     ]
    }
   ],
   "source": [
    "embedding = []\n",
    "for f in feat:\n",
    "    embedding.append(load_w2v(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding_Layer(nn.Module):\n",
    "    def __init__(self,pre_embed, embedding_dim):\n",
    "        super(Embedding_Layer, self).__init__()\n",
    "        self.embedding = nn.Embedding(pre_embed.shape[0], embedding_dim)\n",
    "        self.embedding.weight.data.copy_(torch.Tensor(pre_embed)) \n",
    "        self.embedding.weight.requires_grad = False\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.to(torch.int64)\n",
    "        embedsm = self.embedding(x)\n",
    "        return embedsm \n",
    "\n",
    "class LSTM_MultID_Model(nn.Module):\n",
    "    def __init__(self, class_num, pre_embed, embedding_dim, hidden_dim,bidirectional=True):\n",
    "        super(LSTM_MultID_Model, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        for index,(pre_e,e_size) in enumerate(zip(pre_embed,embedding_dim)):\n",
    "            setattr(self, 'embedding_layer_{}'.format(index), Embedding_Layer(pre_e,e_size))\n",
    "        \n",
    "        self.lstm = nn.LSTM(sum(embedding_dim),hidden_dim, num_layers=1,bidirectional=bidirectional)\n",
    "        if bidirectional==True:\n",
    "            fc_hd = hidden_dim*2\n",
    "        else:\n",
    "            fc_hd = hidden_dim\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(fc_hd, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, class_num)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(len(x)):\n",
    "            x[i] = x[i].to(torch.int64)\n",
    "        embedding_buffer = [getattr(self, 'embedding_layer_{}'.format(index))(inp_embed) for index, inp_embed in enumerate(x)]\n",
    "        embedsm = torch.cat(embedding_buffer,dim=2)\n",
    "        output, hidden = self.lstm(embedsm,None)\n",
    "        output = output.mean(axis=1)\n",
    "        output = self.classifier(output)\n",
    "        output = F.log_softmax(output, dim=1)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM_MultID_Model(\n",
       "  (embedding_layer_0): Embedding_Layer(\n",
       "    (embedding): Embedding(3412773, 128)\n",
       "  )\n",
       "  (embedding_layer_1): Embedding_Layer(\n",
       "    (embedding): Embedding(3027361, 128)\n",
       "  )\n",
       "  (embedding_layer_2): Embedding_Layer(\n",
       "    (embedding): Embedding(57871, 128)\n",
       "  )\n",
       "  (embedding_layer_3): Embedding_Layer(\n",
       "    (embedding): Embedding(39058, 128)\n",
       "  )\n",
       "  (lstm): LSTM(512, 96, bidirectional=True)\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=192, out_features=512, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed=[128,128,128,128]\n",
    "hidden=128\n",
    "age_model = LSTM_MultID_Model(10,pre_embed=embedding,embedding_dim=embed,hidden_dim=hidden,bidirectional=True)\n",
    "age_model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, device, optimizer, epoch):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    \n",
    "    for batch_idx,(data, target) in enumerate(dataloader):\n",
    "        for i in range(len(data)):\n",
    "            data[i] = data[i].to(device)\n",
    "        target = target.to(device) \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target) \n",
    "        loss.backward() \n",
    "        optimizer.step()\n",
    "        train_loss += float(loss.item())\n",
    "        pred = torch.argmax(output,1)\n",
    "        train_acc += pred.eq(target.view_as(pred)).sum().item()\n",
    "        scheduler.step(loss)\n",
    "        \n",
    "        if (batch_idx+1) % 200 == 0:\n",
    "            print('train epoch: {} [{}/{} ({:.0f}%)]\\tacc:{:.6f} \\tloss: {:.6f}'.format(\n",
    "                epoch, (batch_idx+1) * len(data[0]), len(dataloader.dataset),\n",
    "                100. * (batch_idx+1) / len(dataloader), train_acc/((batch_idx+1)*len(data[0])), train_loss/(batch_idx+1)))\n",
    "            \n",
    "def dev(model, device, dev_loader):\n",
    "    model.eval()\n",
    "    dev_loss = 0.0\n",
    "    dev_acc = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data, target in dev_loader:\n",
    "            for i in range(len(data)):\n",
    "                data[i] = data[i].to(device)\n",
    "            target = target.to(device) \n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            dev_loss += float(loss.item())\n",
    "            pred = output.max(1, keepdim = True)[1] \n",
    "            dev_acc += pred.eq(target.view_as(pred)).sum().item()\n",
    "    \n",
    "    dev_loss /= len(dev_loader.dataset)\n",
    "    dev_loss *= BATCH_SIZE\n",
    "    dev_acc  /= len(dev_loader.dataset)\n",
    "    print(\"\\nTest set: loss: {:.4f}, Accuracy:{:.6f}% \\n\".format(\n",
    "        dev_loss,dev_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch: 1 [3200/810000 (0%)]\tacc:0.245625 \tloss: 1.915384\n",
      "train epoch: 1 [6400/810000 (1%)]\tacc:0.273750 \tloss: 1.813156\n",
      "train epoch: 1 [9600/810000 (1%)]\tacc:0.293333 \tloss: 1.749837\n",
      "train epoch: 1 [12800/810000 (2%)]\tacc:0.307812 \tloss: 1.717378\n",
      "train epoch: 1 [16000/810000 (2%)]\tacc:0.315750 \tloss: 1.692146\n",
      "train epoch: 1 [19200/810000 (2%)]\tacc:0.322240 \tloss: 1.673226\n",
      "train epoch: 1 [22400/810000 (3%)]\tacc:0.326205 \tloss: 1.658746\n",
      "train epoch: 1 [25600/810000 (3%)]\tacc:0.330156 \tloss: 1.644793\n",
      "train epoch: 1 [28800/810000 (4%)]\tacc:0.333576 \tloss: 1.632474\n",
      "train epoch: 1 [32000/810000 (4%)]\tacc:0.338719 \tloss: 1.624289\n",
      "train epoch: 1 [35200/810000 (4%)]\tacc:0.340085 \tloss: 1.617194\n",
      "train epoch: 1 [38400/810000 (5%)]\tacc:0.342839 \tloss: 1.608667\n",
      "train epoch: 1 [41600/810000 (5%)]\tacc:0.345385 \tloss: 1.600887\n",
      "train epoch: 1 [44800/810000 (6%)]\tacc:0.348571 \tloss: 1.593917\n",
      "train epoch: 1 [48000/810000 (6%)]\tacc:0.350792 \tloss: 1.589775\n",
      "train epoch: 1 [51200/810000 (6%)]\tacc:0.352578 \tloss: 1.585628\n",
      "train epoch: 1 [54400/810000 (7%)]\tacc:0.354228 \tloss: 1.580692\n",
      "train epoch: 1 [57600/810000 (7%)]\tacc:0.355347 \tloss: 1.575131\n",
      "train epoch: 1 [60800/810000 (8%)]\tacc:0.356168 \tloss: 1.571631\n",
      "train epoch: 1 [64000/810000 (8%)]\tacc:0.358000 \tloss: 1.567680\n",
      "train epoch: 1 [67200/810000 (8%)]\tacc:0.359211 \tloss: 1.563655\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(age_model.parameters(), lr = 1e-2, amsgrad=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min',factor=0.5, patience=4, verbose=True)\n",
    "# scheduler = StepLR(optimizer, step_size=1,gamma=0.1)\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(age_model,train_dataloader,DEVICE,optimizer,epoch)\n",
    "    dev(age_model,DEVICE,dev_dataloader)\n",
    "#     scheduler.step()\n",
    "    torch.save(age_model.state_dict(), './pred_model/mult_bilstm_age_model'+str(epoch)+'.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " torch.save(age_model.state_dict(), './pred_model/mult_bilstm_age_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
